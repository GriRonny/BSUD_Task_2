{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sentiment Analysis using Clustering",
   "id": "1ff4466164779a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing:",
   "id": "678f27c01a1a51a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load and Preprocess Data",
   "id": "bdd4cb133ba6480b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T10:45:55.195340Z",
     "start_time": "2024-12-06T10:45:55.188699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load preprocessed reviews\n",
    "df_processed = pd.read_csv('Data/reviews_preprocessed.csv')\n",
    "\n",
    "print(df_processed.head())\n"
   ],
   "id": "69ef01a0377997d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Tokens     Label\n",
      "0  great music service audio high quality app eas...  positive\n",
      "1  ignore previous negative rating app super grea...  positive\n",
      "2  pop good spotify experience android 12 annoyin...  positive\n",
      "3                        buggy terrible use recently  negative\n",
      "4            dear spotify song playlist shuffle play  negative\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Separate Reviews by Sentiment",
   "id": "bb3ed9c1446d866"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T10:50:16.335687Z",
     "start_time": "2024-12-06T10:50:16.330570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Separate positive and negative reviews based on vader label. This only includes the preprocessed version.\n",
    "positive_reviews = df_processed[df_processed[\"Label\"] == 'positive']\n",
    "neutral_reviews = df_processed[df_processed[\"Label\"] == 'neutral']\n",
    "negative_reviews = df_processed[df_processed[\"Label\"] == 'negative']\n",
    "\n",
    "print(neutral_reviews.head(10))"
   ],
   "id": "8d14d675d90e0d55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Tokens    Label\n",
      "5   player control disappear reason app restart fo...  neutral\n",
      "7   extremely slow change storage external sd card...  neutral\n",
      "17  listen download playlist offline point feature...  neutral\n",
      "22                log acc try open log will open logo  neutral\n",
      "30  option delete song album option download track...  neutral\n",
      "61             dose play like song song look get star  neutral\n",
      "70                                            long ad  neutral\n",
      "73  great song selection amazing audio quality pro...  neutral\n",
      "82  like spotify proplem ad shuffle 6 skip hour so...  neutral\n",
      "85  good u want play song song yo listen instaed s...  neutral\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Extraction",
   "id": "8afebd7b4327ef83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TF-IDF Vectorization: Convert the text data into numerical form for clustering",
   "id": "e346d9e48bf1c4ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T10:51:06.213960Z",
     "start_time": "2024-12-06T10:51:05.178167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.9, min_df=2, stop_words='english')\n",
    "\n",
    "# Positive reviews\n",
    "X_positive = vectorizer.fit_transform(positive_reviews)\n",
    "\n",
    "# Negative reviews\n",
    "X_negative = vectorizer.fit_transform(negative_reviews)\n"
   ],
   "id": "88457b0134e15869",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max_df corresponds to < documents than min_df",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[35], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m vectorizer \u001B[38;5;241m=\u001B[39m TfidfVectorizer(max_df\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.9\u001B[39m, min_df\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, stop_words\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Positive reviews\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m X_positive \u001B[38;5;241m=\u001B[39m \u001B[43mvectorizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpositive_reviews\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Negative reviews\u001B[39;00m\n\u001B[0;32m      9\u001B[0m X_negative \u001B[38;5;241m=\u001B[39m vectorizer\u001B[38;5;241m.\u001B[39mfit_transform(negative_reviews)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python_Elective_FHNW\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2091\u001B[0m, in \u001B[0;36mTfidfVectorizer.fit_transform\u001B[1;34m(self, raw_documents, y)\u001B[0m\n\u001B[0;32m   2084\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_params()\n\u001B[0;32m   2085\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf \u001B[38;5;241m=\u001B[39m TfidfTransformer(\n\u001B[0;32m   2086\u001B[0m     norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm,\n\u001B[0;32m   2087\u001B[0m     use_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_idf,\n\u001B[0;32m   2088\u001B[0m     smooth_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msmooth_idf,\n\u001B[0;32m   2089\u001B[0m     sublinear_tf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msublinear_tf,\n\u001B[0;32m   2090\u001B[0m )\n\u001B[1;32m-> 2091\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2092\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf\u001B[38;5;241m.\u001B[39mfit(X)\n\u001B[0;32m   2093\u001B[0m \u001B[38;5;66;03m# X is already a transformed view of raw_documents so\u001B[39;00m\n\u001B[0;32m   2094\u001B[0m \u001B[38;5;66;03m# we set copy to False\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python_Elective_FHNW\\venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1471\u001B[0m     )\n\u001B[0;32m   1472\u001B[0m ):\n\u001B[1;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python_Elective_FHNW\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1382\u001B[0m, in \u001B[0;36mCountVectorizer.fit_transform\u001B[1;34m(self, raw_documents, y)\u001B[0m\n\u001B[0;32m   1380\u001B[0m min_doc_count \u001B[38;5;241m=\u001B[39m min_df \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(min_df, Integral) \u001B[38;5;28;01melse\u001B[39;00m min_df \u001B[38;5;241m*\u001B[39m n_doc\n\u001B[0;32m   1381\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m max_doc_count \u001B[38;5;241m<\u001B[39m min_doc_count:\n\u001B[1;32m-> 1382\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_df corresponds to < documents than min_df\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1383\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m max_features \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1384\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sort_features(X, vocabulary)\n",
      "\u001B[1;31mValueError\u001B[0m: max_df corresponds to < documents than min_df"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Parameters:\n",
    "\n",
    "**max_df=0.9:** Ignores terms that appear in more than 90% of the documents.\n",
    "**min_df=2:** Ignores terms that appear in less than 2 documents.\n",
    "**stop_words='english':** Removes common English stop words."
   ],
   "id": "97b8152af5bc932b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dimensionality Reduction",
   "id": "59a940ed84798a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We'll reduce dimensions using Truncated Singular Value Decomposition (SVD).",
   "id": "2d107b40721474d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T17:51:23.237790Z",
     "start_time": "2024-11-30T17:51:23.231010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "n_components = 16 # must be <= n_features TODO: Make this dynamic, maybe PCA\n",
    "\n",
    "# Positive reviews\n",
    "svd_positive = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "X_positive_reduced = svd_positive.fit_transform(X_positive)\n",
    "\n",
    "# Negative reviews\n",
    "svd_negative = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "X_negative_reduced = svd_negative.fit_transform(X_negative)\n"
   ],
   "id": "8f3996fc6d0cfc3d",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## The Clustering",
   "id": "f60ecc79023c738d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We'll use K-Means clustering to group the reviews.",
   "id": "19bb2e848bfc3dea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T17:51:25.790311Z",
     "start_time": "2024-11-30T17:51:25.773311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Number of clusters TODO: Check k\n",
    "k = 5\n",
    "\n",
    "# Positive reviews\n",
    "kmeans_positive = KMeans(n_clusters=k, random_state=42)\n",
    "labels_positive = kmeans_positive.fit_predict(X_positive_reduced)\n",
    "\n",
    "# Negative reviews\n",
    "kmeans_negative = KMeans(n_clusters=k, random_state=42)\n",
    "labels_negative = kmeans_negative.fit_predict(X_negative_reduced)\n"
   ],
   "id": "b8c2532d09f02fc0",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cluster Interpretation",
   "id": "bd0bacddd0b02f3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Assign Cluster Labels",
   "id": "1d6dec62ce5f3235"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T17:51:28.303676Z",
     "start_time": "2024-11-30T17:51:28.298712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Positive reviews DataFrame\n",
    "df_positive = df_combined[df_combined['vader_label'] == 'positive'].copy()\n",
    "df_positive['cluster'] = labels_positive\n",
    "\n",
    "# Negative reviews DataFrame\n",
    "df_negative = df_combined[df_combined['vader_label'] == 'negative'].copy()\n",
    "df_negative['cluster'] = labels_negative\n"
   ],
   "id": "a7a17c28619d021e",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Extract Top Terms per Cluster. Compute TF-IDF score for each term in a cluster and extract its top terms.",
   "id": "d58b587d2bbd2ea1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T17:51:31.405609Z",
     "start_time": "2024-11-30T17:51:31.401561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Function to get top terms per cluster\n",
    "def get_top_terms_per_cluster(tfidf_matrix, labels, vectorizer, n_terms=10):\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    cluster_terms = {}\n",
    "\n",
    "    for cluster_num in np.unique(labels):\n",
    "        cluster_indices = np.where(labels == cluster_num)\n",
    "        mean_tfidf = tfidf_matrix[cluster_indices].mean(axis=0)\n",
    "        top_indices = np.argsort(mean_tfidf.A1)[::-1][:n_terms]\n",
    "        top_terms = [terms[i] for i in top_indices]\n",
    "        cluster_terms[cluster_num] = top_terms\n",
    "\n",
    "    return cluster_terms\n"
   ],
   "id": "ab125f7992b5841a",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Display top terms",
   "id": "cfef6fe856ffec50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T17:51:34.335173Z",
     "start_time": "2024-11-30T17:51:34.313557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Top terms in positive clusters\n",
    "top_terms_positive = get_top_terms_per_cluster(X_positive, labels_positive, vectorizer)\n",
    "\n",
    "print(\"Top Terms in Positive Review Clusters:\")\n",
    "for cluster_num, terms in top_terms_positive.items():\n",
    "    print(f\"\\nCluster {cluster_num}: {', '.join(terms)}\")\n"
   ],
   "id": "9473eb0d1e3867fb",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 17 is out of bounds for axis 0 with size 16",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[42], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Top terms in positive clusters\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m top_terms_positive \u001B[38;5;241m=\u001B[39m \u001B[43mget_top_terms_per_cluster\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_positive\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels_positive\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvectorizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTop Terms in Positive Review Clusters:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m cluster_num, terms \u001B[38;5;129;01min\u001B[39;00m top_terms_positive\u001B[38;5;241m.\u001B[39mitems():\n",
      "Cell \u001B[1;32mIn[41], line 13\u001B[0m, in \u001B[0;36mget_top_terms_per_cluster\u001B[1;34m(tfidf_matrix, labels, vectorizer, n_terms)\u001B[0m\n\u001B[0;32m     11\u001B[0m     mean_tfidf \u001B[38;5;241m=\u001B[39m tfidf_matrix[cluster_indices]\u001B[38;5;241m.\u001B[39mmean(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     12\u001B[0m     top_indices \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margsort(mean_tfidf\u001B[38;5;241m.\u001B[39mA1)[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][:n_terms]\n\u001B[1;32m---> 13\u001B[0m     top_terms \u001B[38;5;241m=\u001B[39m [\u001B[43mterms\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m top_indices]\n\u001B[0;32m     14\u001B[0m     cluster_terms[cluster_num] \u001B[38;5;241m=\u001B[39m top_terms\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cluster_terms\n",
      "\u001B[1;31mIndexError\u001B[0m: index 17 is out of bounds for axis 0 with size 16"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ee9774c8c1ca538a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
